---
title: "ローカルモデル概要"
---

<Note>
キャレット(Careti)基準の文書です。Careti v3.38.1 マージ版に準拠し、ローカルランタイム/認証/ルーティング差異があれば `<Note>` で示します。
</Note>

## Careti でローカルモデルを実行

インターネット不要でローカル環境だけで実行できます。API コスト不要、データ外部送信なし。

## クイックスタート

1. **ハードウェア確認** - 32GB RAM 以上
2. **ランタイム選択** - [LM Studio](/ja/running-models-locally/lm-studio) / [Ollama](/ja/running-models-locally/ollama)
3. **Qwen3 Coder 30B をダウンロード**
4. **設定** - コンパクトプロンプト、コンテキスト長
5. **オフラインで開始**

## ハードウェア要件

| RAM | 推奨モデル | 量子化 | パフォーマンス |
| --- | --- | --- | --- |
| 32GB | Qwen3 Coder 30B | 4-bit | 入門ローカルコーディング |
| 64GB | Qwen3 Coder 30B | 8-bit | Careti の全機能 |
| 128GB+ | GLM-4.5-Air | 4-bit | クラウド並み |

## 推奨モデル

### Qwen3 Coder 30B

- **256K コンテキスト**
- **ツール使用が安定**
- **リポジトリ規模の理解**

ダウンロードサイズ:
- 4-bit: 約 17GB
- 8-bit: 約 32GB
- 16-bit: 約 60GB

### 小さいモデルが不向きな理由

- ツール出力が壊れる
- コマンド実行を拒否
- コンテキスト保持が弱い

## ランタイム選択

### LM Studio
- **長所**: GUI、モデル管理が簡単
- **短所**: メモリオーバーヘッド
- [ガイド](/ja/running-models-locally/lm-studio)

### Ollama
- **長所**: CLI、軽量、スクリプト向き
- **短所**: 手動管理
- [ガイド](/ja/running-models-locally/ollama)

## 重要設定

### 必須設定

**Careti:**
- "Use Compact Prompt" を有効化
- 適切なモデル選択
- Base URL を設定

**LM Studio:**
- Context Length: `262144`
- KV Cache Quantization: `OFF`
- Flash Attention: `ON`

**Ollama:**
- `num_ctx 262144`

### 量子化

| 種類 | サイズ削減 | 品質 | 用途 |
| --- | --- | --- | --- |
| 4-bit | 約 75% | 良 | RAM 制限環境 |
| 8-bit | 約 50% | 良 | 専門用途 |
| 16-bit | なし | 最良 | 高 RAM |

### モデル形式

**GGUF**
- 全プラットフォーム
- 量子化選択肢が多い

**MLX**
- Mac 専用
- Apple Silicon 最適化

## パフォーマンス目安

- **初回ロード**: 10〜30秒
- **生成速度**: 5〜20 tokens/s
- **コンテキスト処理**: 大規模ほど遅い
- **メモリ使用量**: 量子化サイズに近い
