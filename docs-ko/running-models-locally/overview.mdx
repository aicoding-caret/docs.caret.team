---
title: "로컬 모델 개요"
---

## Caret에서 로컬 모델 실행하기

인터넷 없이도 강력한 모델을 로컬 하드웨어에서 실행할 수 있습니다. API 비용이 없고, 데이터가 외부로 나가지 않으며, 네트워크 의존성도 없습니다.

로컬 모델은 이제 실제 개발에 충분히 활용 가능한 수준에 도달했습니다. 이 가이드는 Caret를 로컬 모델로 사용하는 데 필요한 핵심 내용을 설명합니다.

## 빠른 시작

1. **하드웨어 확인** - 최소 32GB RAM 권장
2. **런타임 선택** - [LM Studio](/ko/running-models-locally/lm-studio) 또는 [Ollama](/ko/running-models-locally/ollama)
3. **Qwen3 Coder 30B 다운로드** - 추천 모델
4. **설정 구성** - 컴팩트 프롬프트 사용, 컨텍스트 길이 설정
5. **코딩 시작** - 완전 오프라인 실행

## 하드웨어 요구 사항

RAM 크기에 따라 실행 가능한 모델이 달라집니다:

| RAM | 추천 모델 | 양자화 | 성능 수준 |
| --- | --- | --- | --- |
| 32GB | Qwen3 Coder 30B | 4-bit | 기본 로컬 코딩 |
| 64GB | Qwen3 Coder 30B | 8-bit | Caret 전체 기능 |
| 128GB+ | GLM-4.5-Air | 4-bit | 클라우드 수준 성능 |

## 추천 모델

### 1순위: Qwen3 Coder 30B

테스트 결과 **Qwen3 Coder 30B**가 70B 이하 모델 중 Caret와 가장 안정적으로 동작합니다:

- **256K 기본 컨텍스트** - 저장소 전체 처리 가능
- **강력한 도구 사용 능력** - 명령 실행 안정적
- **대규모 코드 이해** - 파일 간 문맥 유지
- **높은 신뢰성** - Caret 도구 포맷과 호환

다운로드 크기:
- 4-bit: 약 17GB (32GB RAM 권장)
- 8-bit: 약 32GB (64GB RAM 권장)
- 16-bit: 약 60GB (128GB+ 필요)

### 왜 더 작은 모델은 안 되나요?

30B 이하(7B~20B) 모델 대부분은 Caret에서 다음 문제가 발생합니다:
- 도구 호출 포맷 오류
- 명령 실행 거부
- 컨텍스트 유지 실패
- 복잡한 코딩 작업 처리 불가

## 런타임 옵션

### LM Studio
- **장점**: GUI 제공, 모델 관리 쉬움, 내장 서버
- **단점**: UI로 인한 메모리 오버헤드, 한 번에 하나의 모델
- **추천 대상**: 간편한 데스크톱 사용자
- [설정 가이드 →](/ko/running-models-locally/lm-studio)

### Ollama
- **장점**: CLI 기반, 낮은 메모리 오버헤드, 스크립트 친화적
- **단점**: 터미널 사용 필요, 모델 관리 수동
- **추천 대상**: 파워 유저/서버 환경
- [설정 가이드 →](/ko/running-models-locally/ollama)

## 중요 설정

### 필수 설정

**Caret 설정:**
- ✅ "Use Compact Prompt" 활성화 (프롬프트 크기 90% 절감)
- ✅ 적절한 모델 선택
- ✅ 로컬 서버에 맞는 Base URL 설정

**LM Studio 설정:**
- Context Length: `262144` (최대)
- KV Cache Quantization: `OFF` (중요)
- Flash Attention: `ON` (하드웨어 지원 시)

**Ollama 설정:**
- 컨텍스트 길이: `num_ctx 262144`
- 하드웨어 지원 시 flash attention 활성화

### 양자화 이해

양자화는 모델 정밀도를 낮춰 소비자 하드웨어에 맞추는 기술입니다:

| 타입 | 용량 감소 | 품질 | 사용 사례 |
| --- | --- | --- | --- |
| 4-bit | 약 75% | 양호 | 대부분의 코딩, RAM 제한 환경 |
| 8-bit | 약 50% | 더 좋음 | 전문 작업, 높은 품질 |
| 16-bit | 없음 | 최고 | 최대 품질, 고RAM 필요 |

### 모델 포맷

**GGUF (범용)**
- 모든 플랫폼 지원(Windows/Linux/Mac)
- 다양한 양자화 옵션
- 넓은 호환성
- 대부분 사용자에게 권장

**MLX (Mac 전용)**
- Apple Silicon 최적화(M1/M2/M3)
- Metal/AMX 가속 활용
- 더 빠른 추론
- macOS 13+ 필요

## 성능 기대치

### 정상적인 범위

- **초기 로드 시간**: 모델 워밍업 10~30초
- **토큰 생성 속도**: 소비자 하드웨어 기준 5~20 tokens/s
- **컨텍스트 처리**: 코드베이스가 클수록 느려짐
- **메모리 사용량**: 양자화 크기와 유사
