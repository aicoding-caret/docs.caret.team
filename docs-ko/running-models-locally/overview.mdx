---
title: "로컬 모델 개요"
---

## Caret을 로컬 모델로 실행하기

Caret을 완전히 오프라인으로 실행할 수 있습니다. API 비용도 없고, 데이터가 외부로 나가지 않으며, 인터넷 연결도 필요 없습니다.

로컬 모델은 이제 실전 개발에 사용할 수 있을 만큼 성능이 올라왔습니다. 이 가이드는 Caret과 함께 로컬 모델을 사용하는 데 필요한 핵심 내용을 정리합니다.

## 빠른 시작

1. **하드웨어 확인** - 최소 32GB RAM
2. **런타임 선택** - [LM Studio](/ko/running-models-locally/lm-studio) 또는 [Ollama](/ko/running-models-locally/ollama)
3. **Qwen3 Coder 30B 다운로드** - 추천 모델
4. **설정 구성** - Compact Prompt 활성화, 컨텍스트 설정
5. **코딩 시작** - 완전 오프라인

## 하드웨어 요구사항

RAM 용량에 따라 실행 가능한 모델이 달라집니다:

| RAM | 추천 모델 | 양자화 | 성능 수준 |
| --- | --- | --- | --- |
| 32GB | Qwen3 Coder 30B | 4-bit | 기본 로컬 코딩 |
| 64GB | Qwen3 Coder 30B | 8-bit | Caret 기능 대부분 사용 |
| 128GB+ | GLM-4.5-Air | 4-bit | 클라우드급 성능 |

## 추천 모델

### 1순위: Qwen3 Coder 30B

테스트 결과, 70B 이하 모델 중 **Qwen3 Coder 30B**가 Caret과 가장 안정적으로 작동합니다:

- **256K 네이티브 컨텍스트 윈도우**
- **강력한 도구 사용 능력**
- **리포지토리 규모 이해**
- **Caret 도구 포맷에 대한 높은 안정성**

다운로드 용량:
- 4-bit: 약 17GB (32GB RAM 권장)
- 8-bit: 약 32GB (64GB RAM 권장)
- 16-bit: 약 60GB (128GB+ RAM 필요)

### 왜 더 작은 모델은 어렵나요?

30B 미만(7B~20B) 모델은 Caret에서 다음 문제가 자주 발생합니다:

- 도구 호출 출력이 깨짐
- 명령 실행 거부
- 컨텍스트 유지 실패
- 복잡한 작업에서 성능 저하

## 런타임 선택

### LM Studio
- **장점**: GUI 기반, 모델 관리 쉬움, 내장 서버
- **단점**: UI로 인한 메모리 오버헤드, 단일 모델 한계
- **추천 대상**: 데스크톱 사용자
- [설정 가이드 →](/ko/running-models-locally/lm-studio)

### Ollama
- **장점**: CLI 기반, 메모리 효율, 자동화에 적합
- **단점**: 터미널 숙련 필요, 수동 모델 관리
- **추천 대상**: 파워 유저, 서버 환경
- [설정 가이드 →](/ko/running-models-locally/ollama)

## 핵심 설정

### 필수 설정

**Caret 설정:**
- ✅ "Use Compact Prompt" 활성화 (프롬프트 90% 축소)
- ✅ 모델 선택
- ✅ Base URL을 로컬 서버 주소로 설정

**LM Studio 설정:**
- Context Length: `262144`
- KV Cache Quantization: `OFF` (필수)
- Flash Attention: `ON` (가능한 경우)

**Ollama 설정:**
- 컨텍스트: `num_ctx 262144`
- Flash Attention 활성화(지원 시)

### 양자화 이해

양자화는 모델을 소비자 하드웨어에 올릴 수 있도록 정밀도를 줄입니다:

| 타입 | 크기 감소 | 품질 | 사용 사례 |
| --- | --- | --- | --- |
| 4-bit | ~75% | Good | 대부분의 코딩 작업, 제한된 RAM |
| 8-bit | ~50% | Better | 전문 작업, 더 높은 품질 |
| 16-bit | 없음 | Best | 최고 품질, 고사양 RAM 필요 |

### 모델 포맷

**GGUF (범용)**
- Windows/Linux/Mac 모두 지원
- 다양한 양자화 옵션
- 호환성 우수

**MLX (Mac 전용)**
- Apple Silicon 최적화
- Metal/AMX 가속 사용
- 빠른 추론
- macOS 13+ 필요

## 성능 기대치

### 일반적인 성능

- **초기 로딩**: 10~30초
- **토큰 생성**: 5~20 tokens/sec
- **컨텍스트 처리**: 코드베이스가 클수록 느림
- **메모리 사용량**: 양자화 크기 수준

### 성능 향상 팁

1. **Compact Prompt 사용**
2. **컨텍스트 크기 제한**
3. **적절한 양자화 선택**
4. **다른 앱 종료**
5. **SSD 사용**

## 사용 사례 비교

### 로컬 모델이 적합한 경우

✅ **추천:**
- 오프라인 개발 환경
- 민감 데이터 프로젝트
- API 비용 부담 없음
- 무제한 실험
- 에어갭 환경

### 클라우드 모델이 나은 경우

☁️ **추천:**
- 256K 초과 대규모 코드베이스
- 수시간 규모 리팩터링
- 팀 단위 일관된 성능 필요
- 최신 모델 기능 필요
- 시간 민감 프로젝트

## 문제 해결

### 자주 발생하는 문제

**"Shell integration unavailable"**
- Caret 설정 → Terminal → Default Terminal Profile에서 bash 선택
- 대부분의 터미널 통합 문제 해결

**"No connection could be made"**
- 서버 실행 확인(LM Studio/Ollama)
- Base URL 확인
- 방화벽 차단 여부 확인
- 기본 포트: LM Studio(1234), Ollama(11434)

**느린 응답/중간 종료**
- 로컬 모델 특성상 정상 범위
- 8-bit 대신 4-bit로 변경
- Compact Prompt 활성화
- 컨텍스트 크기 축소

**혼란/오류**
- LM Studio에서 KV Cache Quantization OFF 확인
- Compact Prompt 활성화 확인
- Context Length 최대값 설정 확인
- RAM 충분한지 확인

### 성능 최적화

**빠른 추론:**
1. 4-bit 양자화
2. Flash Attention 활성화
3. 불필요한 컨텍스트 축소
4. 다른 앱 종료
5. NVMe SSD 사용

**더 나은 품질:**
1. 8-bit 이상 양자화
2. 컨텍스트 최대화
3. 냉각 확보
4. RAM 충분히 할당

## 고급 설정

### 멀티 GPU
여러 GPU가 있다면 레이어를 분산할 수 있습니다:
- LM Studio: 자동 감지
- Ollama: `num_gpu` 설정

### 커스텀 모델
Qwen3 Coder 30B 외에도 다음을 실험할 수 있습니다:
- DeepSeek Coder V2
- Codestral 22B
- StarCoder2 15B
