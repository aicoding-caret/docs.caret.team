---
title: "Groq"
description: "了解如何配置和使用 Groq 的超快推理与 Careti。在 Groq 专用的 LPU 架构上访问来自 OpenAI、Meta、DeepSeek 等的模型。"
---

Groq 通过其定制的 LPU™（语言处理单元）架构提供超快的 AI 推理，专为推理而设计，而非从训练硬件改造而来。Groq 托管来自多个提供商的开源模型，包括 OpenAI、Meta、DeepSeek、Moonshot AI 等。

**网站：** [https://groq.com/](https://groq.com/)

### 获取 API 密钥

1.  **注册/登录：** 前往 [Groq](https://groq.com/) 创建账户或登录。
2.  **导航到控制台：** 前往 [Groq 控制台](https://console.groq.com/) 访问您的仪表板。
3.  **创建密钥：** 导航到 API 密钥部分并创建新的 API 密钥。为您的密钥提供一个描述性名称（例如"Careti"）。
4.  **复制密钥：** 立即复制 API 密钥。您将无法再次查看它。请安全存储。

### 支持的模型

Careti 支持以下 Groq 模型：

-   `llama-3.3-70b-versatile`（Meta）- 具有 131K 上下文的均衡性能
-   `llama-3.1-8b-instant`（Meta）- 具有 131K 上下文的快速推理
-   `openai/gpt-oss-120b`（OpenAI）- 具有 131K 上下文的旗舰模型
-   `openai/gpt-oss-20b`（OpenAI）- 具有 131K 上下文的紧凑模型
-   `moonshotai/kimi-k2-instruct`（Moonshot AI）- 具有提示缓存的 1 万亿参数模型
-   `deepseek-r1-distill-llama-70b`（DeepSeek/Meta）- 推理优化模型
-   `qwen/qwen3-32b`（阿里云）- 增强的问答任务
-   `meta-llama/llama-4-maverick-17b-128e-instruct`（Meta）- 最新的 Llama 4 变体
-   `meta-llama/llama-4-scout-17b-16e-instruct`（Meta）- 最新的 Llama 4 变体

### 在 Careti 中配置

1.  **打开 Careti 设置：** 点击 Careti 面板中的设置图标（⚙️）。
2.  **选择提供商：** 从"API 提供商"下拉菜单中选择"Groq"。
3.  **输入 API 密钥：** 将您的 Groq API 密钥粘贴到"Groq API Key"字段中。
4.  **选择模型：** 从"模型"下拉菜单中选择您想要的模型。

### Groq 的速度革命

Groq 的 LPU 架构相比传统的基于 GPU 的推理提供了几个关键优势：

#### LPU 架构
与从训练工作负载改造而来的 GPU 不同，Groq 的 LPU 是专为推理而设计的。这消除了在传统系统中造成延迟的架构瓶颈。

#### 无与伦比的速度
- **亚毫秒级延迟**，在不同流量、地区和工作负载中保持一致
- **静态调度**，预先计算的执行图消除了运行时协调延迟
- **张量并行性**，针对低延迟单响应优化，而非高吞吐量批处理

#### 质量无妥协
- **TruePoint 数值**仅在不影响准确性的区域降低精度
- **100 位中间累积**确保无损计算
- **战略精度控制**在保持质量的同时实现比 BF16 快 2-4 倍的速度

#### 内存架构
- **SRAM 作为主存储**（非缓存），芯片上有数百兆字节
- **消除 DRAM/HBM 延迟**，困扰传统加速器的问题
- **实现真正的张量并行性**，通过在多个芯片上分割层

在 Groq 的 [LPU 架构博客文章](https://groq.com/blog/inside-the-lpu-deconstructing-groq-speed) 中了解更多关于 Groq 技术的信息。

### 特殊功能

#### 提示缓存
Kimi K2 模型支持提示缓存，可以显著降低重复提示的成本和延迟。

#### 视觉支持
部分模型支持图像输入和视觉能力。查看 Groq 控制台中的模型详细信息以了解具体功能。

#### 推理模型
某些模型（如 DeepSeek 变体）提供增强的推理能力，具有逐步思考过程。

### 提示和注意事项

-   **模型选择：** 根据您的具体用例和性能需求选择模型。
-   **速度优势：** Groq 在单请求延迟方面表现出色，而非高吞吐量批处理。
-   **开源模型提供商：** Groq 在其快速基础设施上托管来自多个提供商（OpenAI、Meta、DeepSeek 等）的开源模型。
-   **上下文窗口：** 大多数模型提供大型上下文窗口（高达 131K tokens），用于包含大量代码和上下文。
-   **定价：** Groq 凭借其速度优势提供有竞争力的定价。查看 [Groq 定价](https://groq.com/pricing) 页面了解当前费率。
-   **速率限制：** Groq 有慷慨的速率限制，但请查看其文档了解基于您使用层级的当前限制。
