---
title: "Ollama"
description: "使用 Careti 设置 Ollama 进行本地 AI 模型执行的快速指南。"
---

### 📋 先决条件

-   Windows、macOS 或 Linux 计算机
-   已在 VS Code 中安装 Careti

### 🚀 设置步骤

#### 1. 安装 Ollama

-   访问 [ollama.com](https://ollama.com)
-   下载并安装适合您操作系统的版本


	<img
		src="https://storage.googleapis.com/cline_public_images/docs/assets/image%20(2)%20(1)%20(1).png"
		alt="Ollama 下载页面"
	/>


#### 2. 选择并下载模型

-   在 [ollama.com/search](https://ollama.com/search) 浏览模型
-   选择模型并复制命令：

    ```bash
    ollama run [模型名称]
    ```


	<img
		src="https://storage.googleapis.com/cline_public_images/docs/assets/ollama-model-grab%20(2).gif"
		alt="在 Ollama 中选择模型"
	/>


-   打开您的终端并运行命令：

    -   示例：

        ```bash
        ollama run llama2
        ```


	<img
		src="https://storage.googleapis.com/cline_public_images/docs/assets/starting-ollama-terminal%20(2).gif"
		alt="在终端中运行 Ollama"
	/>


**✨ 您的模型现在已准备好在 Careti 中使用！**

#### 3. 配置 Careti

1. 打开 VS Code
2. 点击 Careti 设置图标
3. 选择"Ollama"作为 API 提供商
4. 输入配置：
    - 基本 URL：`http://localhost:11434/`（默认值，可以保持不变）
    - 从可用选项中选择模型


	<img
		src="https://storage.googleapis.com/cline_public_images/docs/assets/selecting-ollama-model-caret%20(3).gif"
		alt="使用 Ollama 配置 Careti"
	/>


### ⚠️ 重要注意事项

-   在与 Careti 一起使用之前启动 Ollama
-   保持 Ollama 在后台运行
-   首次模型下载可能需要几分钟

### 🔧 故障排除

如果 Careti 无法连接到 Ollama：

1. 验证 Ollama 正在运行
2. 检查基本 URL 是否正确
3. 确保模型已下载

需要更多信息？阅读 [Ollama 文档](https://github.com/ollama/ollama/blob/main/docs/api.md)。
