---
title: "Requesty"
description: "Erfahren Sie, wie Sie Requesty mit Careti nutzen, um auf über 150 Large Language Models zuzugreifen und diese zu optimieren."
---

Careti unterstützt den Zugriff auf Modelle über die [Requesty](https://www.requesty.ai/) AI-Plattform. Requesty bietet eine einfache und optimierte API für die Interaktion mit über 150 Large Language Models (LLMs).

**Website:** [https://www.requesty.ai/](https://www.requesty.ai/)

### API-Key erhalten

1.  **Registrieren/Anmelden:** Gehen Sie auf die [Requesty-Website](https://www.requesty.ai/) und erstellen Sie ein Konto oder melden Sie sich an.
2.  **API-Key abrufen:** Sie erhalten einen API-Key im Bereich [API Management](https://app.requesty.ai/api-keys) Ihres Requesty-Dashboards.

### Unterstützte Modelle

Requesty bietet Zugriff auf eine breite Palette von Modellen. Careti wird automatisch die aktuellste Liste der verfügbaren Modelle abrufen. Die vollständige Liste der verfügbaren Modelle finden Sie auf der Seite [Model List](https://app.requesty.ai/router/list).

### Konfiguration in Careti

1.  **Careti-Einstellungen öffnen:** Klicken Sie auf das Einstellungs-Icon (⚙️) im Careti-Panel.
2.  **Provider auswählen:** Wählen Sie "Requesty" aus dem Dropdown-Menü "API Provider".
3.  **API-Key eingeben:** Fügen Sie Ihren Requesty API-Key in das Feld "Requesty API Key" ein.
4.  **Modell auswählen:** Wählen Sie Ihr gewünschtes Modell aus dem Dropdown-Menü "Model".

### Tipps und Hinweise

-   **Optimierungen**: Requesty bietet eine Reihe von In-Flight-Kostenoptimierungen an, um Ihre Kosten zu senken.
-   **Einheitliche und vereinfachte Abrechnung**: Uneingeschränkter Zugriff auf alle Provider und Modelle, automatisches Aufladen des Guthabens und mehr über einen einzigen [API-Key](https://app.requesty.ai/api-keys).
-   **Kostenverfolgung**: Verfolgen Sie die Kosten pro Modell, Programmiersprache, geänderter Datei und mehr über das [Cost dashboard](https://app.requesty.ai/cost-management) oder die [Requesty VS Code extension](https://marketplace.visualstudio.com/items?itemName=Requesty.requesty).
-   **Statistiken und Logs**: Sehen Sie sich Ihr [Coding stats dashboard](https://app.requesty.ai/usage-stats) an oder durchsuchen Sie Ihre [LLM interaction logs](https://app.requesty.ai/logs).
-   **Fallback-Richtlinien**: Halten Sie Ihr LLM einsatzbereit mit Fallback-Richtlinien, falls Provider ausfallen.
-   **Prompt Caching:** Einige Provider unterstützen Prompt Caching. [Modelle mit Caching suchen](https://app.requesty.ai/router/list).

### Relevante Ressourcen

-   [Requesty YouTube-Kanal](https://www.youtube.com/@requestyAI)
-   [Requesty Discord](https://requesty.ai/discord)