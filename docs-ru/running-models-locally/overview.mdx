---
title: "Обзор локальных моделей"
---

<Note>
Это документация для Caret. Она основана на версии Caret v3.38.1; специфические политики Caret (поддерживаемые локальные Runtime, аутентификация/маршрутизация, ограничения моделей) будут отмечены в тексте тегом `<Note>`.
</Note>

## Запуск моделей локально с помощью Caret

Запускайте Caret полностью в автономном режиме с действительно мощными моделями на собственном оборудовании. Никаких затрат на API, данные не покидают вашу машину, никакой зависимости от интернета.

Локальные модели достигли переломного момента, когда они стали практичными для реальной разработки. Это руководство охватывает все, что вам нужно знать о работе Caret с локальными моделями.

## Быстрый старт

1. **Проверьте свое оборудование** — минимум 32GB RAM
2. **Выберите Runtime** — [LM Studio](/russian/running-models-locally/lm-studio) или [Ollama](/russian/running-models-locally/ollama)
3. **Скачайте Qwen3 Coder 30B** — рекомендуемая модель
4. **Сконфигурируйте настройки** — включите компактные промпты (compact prompts), установите максимальный контекст
5. **Начните кодить** — полностью оффлайн

## Требования к оборудованию

Ваш объем RAM определяет, какие модели вы сможете эффективно запускать:

| RAM | Рекомендуемая модель | Quantization | Уровень производительности |
| --- | --- | --- | --- |
| 32GB | Qwen3 Coder 30B | 4-bit | Начальный уровень локального кодинга |
| 64GB | Qwen3 Coder 30B | 8-bit | Полный функционал Caret |
| 128GB+ | GLM-4.5-Air | 4-bit | Производительность на уровне облачных решений |

## Рекомендуемые модели

### Основная рекомендация: Qwen3 Coder 30B

После тщательного тестирования **Qwen3 Coder 30B** признана самой надежной моделью с параметрами менее 70B для Caret:

- **Нативное окно контекста (context window) 256K** — работа с целыми репозиториями
- **Сильные возможности использования инструментов (tool-use)** — надежное выполнение команд
- **Понимание масштаба репозитория** — сохранение контекста между файлами
- **Проверенная надежность** — стабильные результаты с форматом инструментов Caret

Размеры для скачивания:
- 4-bit: ~17GB (рекомендуется для 32GB RAM)
- 8-bit: ~32GB (рекомендуется для 64GB RAM)
- 16-bit: ~60GB (требуется 128GB+ RAM)

### Почему не использовать модели меньшего размера?

Большинство моделей с параметрами менее 30B (7B-20B) не справляются с Caret, так как они:
- Генерируют некорректные выходные данные для инструментов (tool-use)
- Отказываются выполнять команды
- Не могут удерживать контекст беседы
- Испытывают трудности со сложными задачами программирования

## Варианты Runtime

### LM Studio
- **Плюсы**: удобный GUI, простое управление моделями, встроенный сервер
- **Минусы**: потребление памяти интерфейсом, ограничение одной моделью за раз
- **Лучше всего для**: пользователей десктопов, ценящих простоту
- [Setup Guide →](/russian/running-models-locally/lm-studio)

### Ollama
- **Плюсы**: работа через командную строку, меньшее потребление памяти, поддержка скриптов
- **Минусы**: требуется навык работы в терминале, ручное управление моделями
- **Лучше всего для**: продвинутых пользователей и развертывания на серверах
- [Setup Guide →](/russian/running-models-locally/ollama)

## Критически важная настройка

### Обязательные настройки

**В Caret:**
- ✅ Включите "Use Compact Prompt" — сокращает размер промпта на 90%
- ✅ Выберите подходящую модель в настройках
- ✅ Настройте Base URL в соответствии с вашим сервером

**В LM Studio:**
- Context Length: `262144` (максимум)
- KV Cache Quantization: `OFF` (критично для корректной работы)
- Flash Attention: `ON` (если поддерживается оборудованием)

**В Ollama:**
- Установите окно контекста: `num_ctx 262144`
- Включите flash attention, если поддерживается

### Что такое Quantization

Quantization снижает точность модели, чтобы она могла уместиться на пользовательском оборудовании:

| Тип | Уменьшение размера | Качество | Сценарий использования |
| --- | --- | --- | --- |
| 4-bit | ~75% | Хорошее | Большинство задач кодинга, ограниченная RAM |
| 8-bit | ~50% | Лучше | Профессиональная работа, больше нюансов |
| 16-bit | Отсутствует | Лучшее | Максимальное качество, требуется большой объем RAM |

### Форматы моделей

**GGUF (Universal)**
- Работает на всех платформах (Windows, Linux, Mac)
- Широкие возможности Quantization
- Обширная совместимость с инструментами
- Рекомендуется для большинства пользователей

**MLX (Mac only)**
- Оптимизирован для Apple Silicon (M1/M2/M3)
- Использует ускорение Metal и AMX
- Более быстрый Inference на Mac
- Требуется macOS 13+

## Ожидания по производительности

### Что считается нормой

- **Время начальной загрузки**: 10–30 секунд для прогрева модели
- **Генерация токенов**: 5–20 токенов/сек на пользовательском оборудовании
- **Обработка контекста**: медленнее при работе с большими кодовыми базами
- **Использование памяти**: близко к размеру выбранной Quantization

### Советы по производительности

1. **Используйте компактные промпты (compact prompts)** — необходимо для локального Inference
2. **По возможности ограничивайте контекст** — начинайте с меньших окон
3. **Выбирайте правильную Quantization** — баланс качества и скорости
4. **Закройте другие приложения** — освободите RAM для модели
5. **Используйте SSD** — для более быстрой загрузки модели

## Сравнение сценариев использования

### Когда использовать локальные модели

✅ **Идеально для:**
- Оффлайн сред разработки
- Проектов с высокими требованиями к конфиденциальности
- Обучения без затрат на API
- Неограниченных экспериментов
- Изолированных (Air-gapped) сред
- Разработки с ограниченным бюджетом

### Когда использовать облачные модели

☁️ **Лучше для:**
- Очень больших кодовых баз (>256K токенов)
- Многочасовых сессий рефакторинга
- Команд, которым нужна стабильная производительность
- Доступа к новейшим возможностям моделей
- Проектов с жесткими временными рамками

## Устранение неполадок

### Частые проблемы и решения

**"Shell integration unavailable"**
- Переключитесь на bash в Caret Settings → Terminal → Default Terminal Profile
- Это решает 90% проблем с интеграцией терминала

**"No connection could be made"**
- Проверьте, запущен ли сервер (LM Studio или Ollama)
- Убедитесь, что Base URL совпадает с адресом сервера
- Убедитесь, что Firewall не блокирует соединение
- Порты по умолчанию: LM Studio (1234), Ollama (11434)

**Медленные или неполные ответы**
- Это нормально для локальных моделей (обычно 5–20 токенов/сек)
- Попробуйте меньшую Quantization (4-bit вместо 8-bit)
- Включите компактные промпты, если еще не сделали этого
- Уменьшите размер окна контекста (context window)

**Ошибки или путаница в ответах модели**
- Убедитесь, что KV Cache Quantization выключена (OFF) в LM Studio
- Убедитесь, что включены компактные промпты
- Проверьте, установлен ли максимальный размер контекста
- Убедитесь, что RAM достаточно для выбранной Quantization

### Оптимизация производительности

**Для ускорения Inference:**
1. Используйте 4-bit Quantization
2. Включите Flash Attention
3. Уменьшите окно контекста, если оно не требуется в полном объеме
4. Закройте ненужные приложения
5. Используйте NVMe SSD для хранения моделей

**Для повышения качества:**
1. Используйте 8-bit Quantization или выше
2. Максимизируйте окно контекста
3. Обеспечьте адекватное охлаждение
4. Выделите максимум RAM для модели

## Расширенная настройка

### Конфигурация с несколькими GPU
Если у вас несколько GPU, вы можете распределить слои модели:
- LM Studio: автоматическое определение GPU
- Ollama: настройка параметра `num_gpu`

### Пользовательские модели
Хотя Qwen3 Coder 30B рекомендуется, вы можете поэкспериментировать с:
- DeepSeek Coder V2
- Codestral 22B
- StarCoder2 15B

Примечание: они могут потребовать дополнительной настройки и тестирования.

## Сообщество и поддержка

- **Discord**: [Присоединяйтесь к нашему сообществу](https://https://discord.gg/WB6yaR89YN) для помощи в реальном времени
- **Reddit**: [r/caret](https://www.reddit.com/r/CLine/) для обсуждений
- **GitHub**: [Сообщить о проблеме](https://github.com/caret/caret/issues)

## Дальнейшие шаги

Готовы начать? Выберите свой путь:

<CardGroup cols={2}>
  <Card title="Настройка LM Studio" icon="desktop" href="/russian/running-models-locally/lm-studio">
    Подход с использованием дружелюбного GUI и подробным руководством по настройке
  </Card>
  <Card title="Настройка Ollama" icon="terminal" href="/russian/running-models-locally/ollama">
    Настройка через командную строку для продвинутых пользователей и автоматизации
  </Card>
</CardGroup>

## Резюме

Локальные модели в связке с Caret теперь стали по-настоящему практичными. Хотя они не могут сравниться с топовыми облачными API по скорости, они обеспечивают полную конфиденциальность, отсутствие затрат и возможность работы оффлайн. При правильной настройке и подходящем оборудовании Qwen3 Coder 30B эффективно справляется с большинством задач по написанию кода.

Ключ к успеху — правильная подготовка: достаточный объем RAM, верная конфигурация и реалистичные ожидания. Следуйте этому руководству, и вы получите мощного помощника для кодинга, работающего полностью на вашем железе.