---
title: "Ollama"
description: "Краткое руководство по настройке Ollama для локального запуска моделей ИИ в Careti."
---

<Note>
Это документация для Careti. Она основана на версии Careti v3.38.1. Если существуют специфические политики Careti (поддерживаемые локальные среды исполнения, аутентификация/маршрутизация, ограничения моделей), они будут отмечены в тексте как `<Note>`.
</Note>

### Предварительные условия

-   Компьютер под управлением Windows, macOS или Linux
-   Установленное расширение Careti в VS Code

### Инструкция по настройке

#### 1. Установите Ollama

-   Перейдите на [ollama.com](https://ollama.com)
-   Скачайте и установите версию для вашей операционной системы

<Frame>
	<img
		src="https://storage.googleapis.com/cline_public_images/docs/assets/image%20(2)%20(1)%20(1).png"
		alt="Страница загрузки Ollama"
	/>
</Frame>

#### 2. Выберите и скачайте модель

-   Выберите подходящую модель на [ollama.com/search](https://ollama.com/search)
-   Выберите модель и скопируйте команду:

    ```bash
    ollama run [model-name]
    ```

<Frame>
	<img
		src="https://storage.googleapis.com/cline_public_images/docs/assets/ollama-model-grab%20(2).gif"
		alt="Выбор модели в Ollama"
	/>
</Frame>

-   Откройте Terminal и выполните команду:

    -   Пример:

        ```bash
        ollama run llama2
        ```

<Frame>
	<img
		src="https://storage.googleapis.com/cline_public_images/docs/assets/starting-ollama-terminal%20(2).gif"
		alt="Запуск Ollama в Terminal"
	/>
</Frame>

Теперь ваша модель готова к использованию в Careti.

#### 3. Настройте Careti

<Frame>
	<img
		src="https://storage.googleapis.com/cline_public_images/docs/assets/ollama-setup.gif"
		alt="Полный процесс настройки Ollama"
	/>
</Frame>

Откройте VS Code и настройте Careti:

1. Нажмите на иконку настроек Careti
2. Выберите «Ollama» в качестве API провайдера
3. Base URL: `http://localhost:11434/` (по умолчанию, обычно менять не требуется)
4. Выберите вашу модель из выпадающего списка

### Рекомендуемые модели

Для наилучшей работы с Careti используйте **Qwen3 Coder 30B**. Эта модель обладает отличными навыками написания кода и надежно работает с инструментами при локальной разработке.

Чтобы скачать её:
```bash
ollama run qwen3-coder-30b
```

Другие подходящие модели включают:
- `mistral-small` — хороший баланс производительности и скорости
- `devstral-small` — оптимизирована для задач программирования

### Важные примечания

-   Запустите Ollama перед использованием Careti
-   Оставьте Ollama работать в фоновом режиме
-   Первая загрузка модели может занять несколько минут

### Включите компактные промпты (Compact Prompts)

Для повышения производительности при работе с локальными моделями включите компактные промпты в настройках Careti. Это уменьшит размер промпта на 90%, сохраняя при этом основные функции.

Перейдите в Careti Settings → Features → Use Compact Prompt и включите этот параметр.

### Устранение неполадок

Если Careti не может подключиться к Ollama:

1. Убедитесь, что Ollama запущена
2. Проверьте правильность Base URL
3. Убедитесь, что модель скачана

Нужно больше информации? Ознакомьтесь с [документацией Ollama](https://github.com/ollama/ollama/blob/main/docs/api.md).