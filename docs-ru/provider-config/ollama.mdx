---
title: "Ollama"
---

Caret поддерживает локальный запуск моделей с помощью Ollama. Этот подход обеспечивает приватность, автономный доступ и потенциальное снижение затрат. Для этого требуется первоначальная настройка и достаточно мощный компьютер. Учитывая текущее состояние потребительского оборудования, не рекомендуется использовать Ollama с Caret, так как производительность на средних конфигурациях железа, скорее всего, будет низкой.

**Веб-сайт:** [https://ollama.com/](https://ollama.com/)

### Настройка Ollama

1.  **Скачивание и установка Ollama:**
    Загрузите установщик Ollama для вашей операционной системы с [веб-сайта Ollama](https://ollama.com/) и следуйте их руководству по установке. Убедитесь, что Ollama запущена. Обычно её можно запустить командой:

    ```bash
    ollama serve
    ```

2.  **Загрузка модели:**
    Ollama поддерживает множество различных моделей. Список доступных моделей можно найти в [библиотеке моделей Ollama](https://ollama.com/library). Некоторые модели, рекомендуемые для задач программирования:

    -   `codellama:7b-code` (хорошая и легкая модель для старта)
    -   `codellama:13b-code` (обеспечивает лучшее качество, больший размер)
    -   `codellama:34b-code` (обеспечивает еще более высокое качество, очень большая)
    -   `qwen2.5-coder:32b`
    -   `mistralai/Mistral-7B-Instruct-v0.1` (надежная модель общего назначения)
    -   `deepseek-coder:6.7b-base` (эффективна для кодинга)
    -   `llama3:8b-instruct-q5_1` (подходит для общих задач)

    Чтобы загрузить модель, откройте терминал и выполните:

    ```bash
    ollama pull <model_name>
    ```

    Например:

    ```bash
    ollama pull qwen2.5-coder:32b
    ```

3.  **Настройка окна контекста модели:**
    По умолчанию модели Ollama часто используют окно контекста в 2048 токенов, чего может быть недостаточно для многих запросов Caret. Для получения приемлемых результатов рекомендуется минимум 12 000 токенов, а идеальным вариантом будет 32 000 токенов. Чтобы изменить этот параметр, вам потребуется модифицировать параметры модели и сохранить её как новую версию.

    Сначала загрузите модель (на примере `qwen2.5-coder:32b`):

    ```bash
    ollama run qwen2.5-coder:32b
    ```

    После загрузки модели в интерактивной сессии Ollama установите параметр размера контекста:

    ```
    /set parameter num_ctx 32768
    ```

    Затем сохраните настроенную модель под новым именем:

    ```
    /save your_custom_model_name
    ```

    (Замените `your_custom_model_name` на любое имя по вашему выбору.)

4.  **Настройка Caret:**
    -   Откройте боковую панель Caret (обычно обозначается иконкой Caret).
    -   Нажмите на иконку шестерёнки настроек (⚙️).
    -   Выберите «ollama» в качестве API Provider.
    -   Введите Model name, которое вы сохранили на предыдущем шаге (например, `your_custom_model_name`).
    -   (Опционально) Измените base URL, если Ollama запущена на другом компьютере или порту. Значение по умолчанию: `http://localhost:11434`.
    -   (Опционально) Настройте Model context size в Advanced настройках Caret. Это поможет Caret эффективно управлять окном контекста вашей кастомной модели Ollama.

### Советы и примечания

-   **Требования к ресурсам:** Локальный запуск больших языковых моделей может быть требовательным к системным ресурсам. Убедитесь, что ваш компьютер соответствует требованиям выбранной модели.
-   **Выбор модели:** Экспериментируйте с различными моделями, чтобы найти ту, которая лучше всего подходит для ваших задач и предпочтений.
-   **Автономная работа:** После загрузки модели вы сможете использовать Caret с этой моделью даже без подключения к интернету.
-   **Отслеживание использования токенов:** Caret отслеживает использование токенов для моделей, доступ к которым осуществляется через Ollama, что позволяет вам контролировать потребление.
-   **Собственная документация Ollama:** Для получения более подробной информации обратитесь к официальной [документации Ollama](https://ollama.com/docs).