---
title: "Groq"
description: "Узнайте, как настроить и использовать сверхбыстрый инференс Groq в Caret. Получите доступ к моделям от OpenAI, Meta, DeepSeek и других на базе специализированной архитектуры LPU от Groq."
---

Groq обеспечивает сверхбыстрый AI inference благодаря собственной архитектуре LPU™ (Language Processing Unit), специально разработанной для инференса, а не адаптированной из оборудования для обучения. Groq размещает модели с открытым исходным кодом от различных поставщиков, включая OpenAI, Meta, DeepSeek, Moonshot AI и других.

**Сайт:** [https://groq.com/](https://groq.com/)

### Получение API Key

1.  **Регистрация/Вход:** Перейдите на [Groq](https://groq.com/) и создайте аккаунт или войдите в систему.
2.  **Переход в Console:** Перейдите в [Groq Console](https://console.groq.com/), чтобы получить доступ к панели управления.
3.  **Создание ключа:** Перейдите в раздел API Keys и создайте новый API key. Дайте вашему ключу описательное имя (например, "Caret").
4.  **Копирование ключа:** Немедленно скопируйте API key. Вы не сможете увидеть его снова. Храните его в надежном месте.

### Поддерживаемые модели

Caret поддерживает следующие модели Groq:

-   `llama-3.3-70b-versatile` (Meta) — сбалансированная производительность с контекстом 131K
-   `llama-3.1-8b-instant` (Meta) — быстрый инференс с контекстом 131K  
-   `openai/gpt-oss-120b` (OpenAI) — флагманская модель с контекстом 131K
-   `openai/gpt-oss-20b` (OpenAI) — компактная модель с контекстом 131K
-   `moonshotai/kimi-k2-instruct` (Moonshot AI) — модель с 1 триллионом параметров и поддержкой prompt caching
-   `deepseek-r1-distill-llama-70b` (DeepSeek/Meta) — модель, оптимизированная для рассуждений (reasoning)
-   `qwen/qwen3-32b` (Alibaba Cloud) — улучшена для задач Q&A
-   `meta-llama/llama-4-maverick-17b-128e-instruct` (Meta) — новейший вариант Llama 4
-   `meta-llama/llama-4-scout-17b-16e-instruct` (Meta) — новейший вариант Llama 4

### Настройка в Caret

1.  **Откройте настройки Caret:** Нажмите на иконку настроек (⚙️) в панели Caret.
2.  **Выберите провайдера:** Выберите "Groq" в выпадающем списке "API Provider".
3.  **Введите API Key:** Вставьте ваш Groq API key в поле "Groq API Key".
4.  **Выберите модель:** Выберите нужную модель в выпадающем списке "Model".

### Революция скорости Groq

Архитектура LPU от Groq обеспечивает несколько ключевых преимуществ по сравнению с традиционным инференсом на базе GPU:

#### Архитектура LPU
В отличие от GPU, которые адаптированы под задачи обучения, LPU от Groq специально создан для инференса. Это устраняет архитектурные «узкие места», создающие задержки в традиционных системах.

#### Непревзойденная скорость
- **Задержка менее миллисекунды (sub-millisecond latency)**, которая остается стабильной независимо от трафика, регионов и рабочих нагрузок.
- **Статическое планирование (static scheduling)** с предварительно вычисленными графами выполнения исключает задержки координации во время выполнения.
- **Тензорный параллелизм (tensor parallelism)**, оптимизированный для низких задержек при одиночных ответах, а не для пакетной обработки с высокой пропускной способностью.

#### Качество без компромиссов
- **Числа TruePoint** снижают точность только в тех областях, которые не влияют на достоверность.
- **100-битное промежуточное накопление** обеспечивает вычисления без потерь.
- **Стратегический контроль точности** сохраняет качество, достигая ускорения в 2–4 раза по сравнению с BF16.

#### Архитектура памяти
- **SRAM как основное хранилище** (а не кэш) с сотнями мегабайт на чипе.
- **Устранение задержек DRAM/HBM**, которые характерны для традиционных ускорителей.
- **Обеспечение истинного тензорного параллелизма** за счет разделения слоев между несколькими чипами.

Узнайте больше о технологии Groq в их [блоге об архитектуре LPU](https://groq.com/blog/inside-the-lpu-deconstructing-groq-speed).

### Специальные возможности

#### Prompt Caching
Модель Kimi K2 поддерживает prompt caching, что может значительно снизить затраты и задержки при повторных запросах.

#### Поддержка Vision
Определенные модели поддерживают ввод изображений и возможности компьютерного зрения. Проверьте детали в Groq Console для уточнения возможностей конкретных моделей.

#### Модели для рассуждений (Reasoning Models)
Некоторые модели, такие как варианты DeepSeek, предлагают расширенные возможности рассуждения с пошаговым процессом мышления.

### Советы и примечания

-   **Выбор модели:** Выбирайте модели исходя из вашего конкретного случая использования и требований к производительности.
-   **Преимущество в скорости:** Groq превосходит других в задержке одиночных запросов, а не в пакетной обработке с высокой пропускной способностью.
-   **Провайдер OSS-моделей:** Groq размещает модели с открытым исходным кодом от нескольких поставщиков (OpenAI, Meta, DeepSeek и др.) на своей быстрой инфраструктуре.
-   **Окна контекста:** Большинство моделей предлагают большие окна контекста (до 131K токенов) для включения значительного объема кода и контекста.
-   **Цены:** Groq предлагает конкурентоспособные цены в сочетании со своими преимуществами в скорости. Актуальные тарифы можно найти на странице [Groq Pricing](https://groq.com/pricing).
-   **Rate Limits:** Groq предоставляет щедрые лимиты частоты запросов, но проверяйте их документацию для получения актуальной информации в зависимости от вашего уровня использования.