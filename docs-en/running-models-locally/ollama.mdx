---
title: "Ollama"
description: "A quick guide to setting up Ollama for local AI model execution with Careti."
---

<Note>
캐러티(Careti) 기준 문서입니다. Careti v3.38.1 머지본을 따르며, 캐러티 전용 정책(지원 로컬 런타임, 인증/라우팅, 모델 제한)이 있을 경우 본문에서 `<Note>`로 표시합니다.
</Note>

### Prerequisites

-   Windows, macOS, or Linux computer
-   Careti installed in VS Code

### Setup Steps

#### 1. Install Ollama

-   Visit [ollama.com](https://ollama.com)
-   Download and install for your operating system

<Frame>
	<img
		src="https://storage.googleapis.com/cline_public_images/docs/assets/image%20(2)%20(1)%20(1).png"
		alt="Ollama download page"
	/>
</Frame>

#### 2. Choose and Download a Model

-   Browse models at [ollama.com/search](https://ollama.com/search)
-   Select model and copy command:

    ```bash
    ollama run [model-name]
    ```

<Frame>
	<img
		src="https://storage.googleapis.com/cline_public_images/docs/assets/ollama-model-grab%20(2).gif"
		alt="Selecting a model in Ollama"
	/>
</Frame>

-   Open your Terminal and run the command:

    -   Example:

        ```bash
        ollama run llama2
        ```

<Frame>
	<img
		src="https://storage.googleapis.com/cline_public_images/docs/assets/starting-ollama-terminal%20(2).gif"
		alt="Running Ollama in terminal"
	/>
</Frame>

Your model is now ready to use within Careti.

#### 3. Configure Careti

<Frame>
	<img
		src="https://storage.googleapis.com/cline_public_images/docs/assets/ollama-setup.gif"
		alt="Complete Ollama setup process"
	/>
</Frame>

Open VS Code and configure Careti:

1. Click the Careti settings icon
2. Select "Ollama" as your API provider
3. Base URL: `http://localhost:11434/` (default, usually no need to change)
4. Select your model from the dropdown

### Recommended Models

For the best experience with Careti, use **Qwen3 Coder 30B**. This model provides strong coding capabilities and reliable tool use for local development.

To download it:
```bash
ollama run qwen3-coder-30b
```

Other capable models include:
- `mistral-small` - Good balance of performance and speed
- `devstral-small` - Optimized for coding tasks

### Important Notes

-   Start Ollama before using with Careti
-   Keep Ollama running in background
-   First model download may take several minutes

### Enable Compact Prompts

For better performance with local models, enable compact prompts in Careti settings. This reduces the prompt size by 90% while maintaining core functionality.

Navigate to Careti Settings → Features → Use Compact Prompt and toggle it on.

### Troubleshooting

If Careti can't connect to Ollama:

1. Verify Ollama is running
2. Check base URL is correct
3. Ensure model is downloaded

Need more info? Read the [Ollama Docs](https://github.com/ollama/ollama/blob/main/docs/api.md).
